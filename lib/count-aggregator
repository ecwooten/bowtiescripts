#!/usr/bin/env node

var Path = require('path')
  , Optionall = require('optionall')
  , FSTK = require('fstk')
  , Async = require('async')
  , _ = require('underscore')
  , Belt = require('jsbelt')
  , Util = require('util')
  , Winston = require('winston')
  , Events = require('events')
  , Spinner = require('its-thinking')
  , CP = require('child_process')
  , OS = require('os')
  , CSV = require('fast-csv')
;

var O = new Optionall({
                       '__dirname': Path.resolve(module.filename + '/../..')
                     , 'file_priority': ['package.json']
                     });

var Log = new Winston.Logger();
Log.add(Winston.transports.Console, {'level': 'debug', 'colorize': true, 'timestamp': false});

var Spin = new Spinner(4);

var GB = _.defaults(O.argv, {
  //reads_list
  //count_files_list
  //count_files_path
  //output_path
  'concurrency': OS.cpus().length
});

Spin.start();

Async.waterfall([
  function(cb){
    //required parameters
    _.each([
      'reads_list'
    , 'count_files_list'
    , 'count_files_path'
    , 'output_path'
    ], function(p){
      if (!GB[p]){
        GB.usage = true; //exit and show usage
        Log.error(p + ' is required');
      }
    });

    //print usage message and exit
    if (GB.h || GB.help || GB.usage){
      Log.info([
      , ''
      , '/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////'
      , 'count-aggregator: count bowtie reads across multiple samples and aggregate into a csv file'
      , '/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////'
      , ''
      , 'required arguments:'
      , '\t--reads_list - csv file including a list of reads to aggregate (format: single column, titled "read")'
      , '\t--count_files_list - csv file including list of samples to include in aggregated counts file (format: column titled "sample" [required] with name of sample [lane is optional, multiple lanes of the same sample will be consolidated in output file], column titled "name" [optional] with a user-specified name for the sample)'
      , '\t--count_files_path - path to directory containing .csv or .json files with bowtie read counts (generated by bowtie-counter script)'
      , '\t--output_path - path to save csv file of aggregated counts to, (format: column titled "read" includes each read, columns for each sample [labelled with user-supplied name - if provided - and file name of sample], containing number of times each read appeared in the sample)'
      , ''
      , 'optional arguments:'
      , ''
      , '\t--concurrency - number of count files to tabulate at once (default: number of CPU cores on current system - ' + OS.cpus().length + ')'
      , '\t--help|h|usage - print usage'
      , ''
      , ''
      ].join('\n'));
      return process.exit();
    }

    return FSTK._fs.readdir(GB.counts_files_path, Belt.cs(cb, GB, 'count_files', 1, 0));
  }
, function(cb){
    GB.count_files = _.filter(GB.count_files, function(f){ return f.match(/(\.csv|\.json)$/); });

    Log.info('Getting list of count files to include...');

    GB['samples'] = {};

    return FSTK._fs.createReadStream(GB.count_files_list)
               .pipe(CSV({'headers': true}))
               .on('data', function(data){
                  data['sample'] = data.filename.split(/_L\d+_/).shift();
                  GB.samples[data.sample] = data;
               })
               .on('end', Belt.cw(cb));
  }
, function(cb){
    Log.info('...' + _.size(GB.samples) + ' samples found...');

    var sreg = new RegExp('^(' + _.keys(GB.samples).join('|') + ')');
    GB.count_files = _.filter(GB.count_files, function(f){ return f.match(sreg); });

    Log.info('...' + GB.count_files.length + ' count files to be used...');

    return cb();
  }
, function(cb){
    Log.info('Generating list of reads to tabulate...');

    GB['reads'] = {};

    return FSTK._fs.createReadStream(GB.reads_list)
               .pipe(CSV({'headers': true}))
               .on('data', function(data){
                 _.each(GB.samples, function(s){
                   var name = s.name ? s.name + ' (' + s.sample + ')' : s.sample;
                   data[name] = 0;
                 });
                 GB.reads[data.read] = data;
               })
               .on('end', Belt.cw(cb));
  }
, function(cb){
    Log.info('...tabulating counts for ' + _.size(GB.reads) + ' reads across '
            + _.size(GB.samples) + ' samples in ' + GB.count_files.length + ' count files...');

    var lanes = _.object(_.keys(GB.samples), _.map(GB.samples, function(s){
      return {};
    }));

    return Async.eachLimit(GB.count_files, GB.concurrency, function(f, cb2){
      var sample = f.split(/_L\d+_/).shift()
        , lane = String(Belt.cast(f.split(/_L0/).pop().split('_').shift(), 'number'))
        , name = GB.samples[sample].name ? GB.samples[sample].name + ' (' + GB.samples[sample].sample + ')'
                                         : GB.samples[sample].sample;

      if (lanes[sample][lane]) return cb2();
      lanes[sample][lane] = true;

      Log.info('Adding counts for file "' + f + '", sample [' + sample + '], lane [' + lane + '], labelled as "' + name + '"...');

      if (f.match(/\.json$/)) return FSTK.readJSON(Path.join(GB.count_files_path, '/' + f), function(err, counts){
        if (err) return cb2(err);

        _.each(GB.reads, function(r){
          r[name] += (counts[r.read] || 0);
        });

        return cb2();
      });

      return FSTK._fs.createReadStream(Path.join(GB.count_files_path, '/' + f))
                 .pipe(CSV({'headers': true}))
                 .on('data', function(data){
                   if (!GB.reads[data.read]) return;

                   GB.reads[data.read][name] += (Belt.cast(data.count, 'number') || 0);
                 })
                 .on('end', Belt.cw(cb2));
    }, Belt.cw(cb, 0));
  }
, function(cb){
    Log.info('Writing output file "' + GB.output_path + '"...');

    var output = FSTK._fs.createWriteStream(GB.output_path)
      , csv = CSV.createWriteStream({'headers': true});

    output.on('finish', Belt.cw(cb));

    csv.pipe(output);

    _.each(GB.reads, function(b){ return csv.write(b); });

    return csv.end();
  }
], function(err){
  Spin.stop();
  if (err) Log.error(err);
  return process.exit(err ? 1 : 0);
});
